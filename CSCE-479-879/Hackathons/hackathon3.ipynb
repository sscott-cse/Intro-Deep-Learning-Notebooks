{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCE 479/879 Hackathon: Saving/Loading, Regularization, and Batch Runs\n",
    "\n",
    "Written by Eleanor Quint\n",
    "\n",
    "Modified by Mrinal Rawool and Stephen Scott\n",
    "\n",
    "Topics:\n",
    "- Subclassing `tf.module`\n",
    "- Saving and loading TensorFlow models\n",
    "- Running TensorFlow-based Python programs in batch mode on the command line\n",
    "- Overfitting, regularization, and early stopping\n",
    "\n",
    "This is all setup in a IPython notebook so you can run any code you want to experiment with. Feel free to edit any cell, or add some to run your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start with our library imports...\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np                 # to use numpy arrays\n",
    "import tensorflow as tf            # to specify and run computation graphs\n",
    "import tensorflow_datasets as tfds # to load training data\n",
    "import matplotlib.pyplot as plt    # to visualize data and draw plots\n",
    "from tqdm import tqdm              # to track progress of loops\n",
    "\n",
    "DATA_DIR = './tensorflow-datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subclassing `tf.module`\n",
    "\n",
    "The most flexible way to specify a model is by subclassing `tf.module`. \n",
    "\n",
    "This allows a model to be specified in a general way. \n",
    "\n",
    "The key function to implement is `__call__` which should take in data, run the model forward, and return the output. The function will frequently be decorated with `tf.function` (when not debugging the model), so that it can be compiled and run more quickly.\n",
    "\n",
    "An example implementing a dense layer (generally you should use `tf.keras.layers.Dense`):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is tf.function()?\n",
    "\n",
    "Eager Mode: Used for debugging. Enabled in TF2.0 by default.\n",
    "tf.function() or Graph mode: Faster computation\n",
    "\n",
    "[tf.function](https://www.tensorflow.org/api_docs/python/tf/function)\n",
    "\n",
    "[Tensorflow Graphs](https://www.tensorflow.org/guide/intro_to_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a simple cube function with two concrete signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function \n",
    "def tf_cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConcreteFunction tf_cube(x)\n",
      "  Args:\n",
      "    x: float32 Tensor, shape=()\n",
      "  Returns:\n",
      "    float32 Tensor, shape=()\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "concrete_function_fp = tf_cube.get_concrete_function(tf.constant(2.0))\n",
    "print(concrete_function_fp)\n",
    "print(concrete_function_fp(tf.constant(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConcreteFunction tf_cube(x)\n",
      "  Args:\n",
      "    x: int32 Tensor, shape=()\n",
      "  Returns:\n",
      "    int32 Tensor, shape=()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function_int = tf_cube.get_concrete_function(tf.constant(2))\n",
    "print(concrete_function_int)\n",
    "concrete_function_int(tf.constant(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concrete_function_fp(tf.constant(2)) # check this out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the \"__call__()\" of a layer contains computation, let's decorate \"__call__()\" with tf.function decorator. This will cause a concrete function to be created the first time the layer is called. Any future calls to this layer will reuse the computation graph so long as the input types match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "X = tf.ones([32,3])\n",
    "print(X.shape)\n",
    "print(X.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'b:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'w:0' shape=(3, 10) dtype=float32, numpy=\n",
       " array([[ 0.12225031, -1.8198787 , -0.6985519 , -1.448438  ,  0.13599414,\n",
       "          1.350059  , -1.1127319 ,  2.2352157 ,  0.453757  ,  1.013791  ],\n",
       "        [ 0.7127462 ,  0.61658096, -0.0058486 ,  0.8308272 , -0.21494398,\n",
       "          1.2897937 , -0.10022853, -1.2402375 , -1.00956   , -0.86073035],\n",
       "        [ 0.20583752, -0.48725078,  0.42969358,  0.1832197 ,  0.0678944 ,\n",
       "         -1.1582944 ,  1.6698463 ,  0.6377107 , -0.01898999,  1.7055923 ]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dense(tf.Module):\n",
    "    def __init__(self, output_size, activation=tf.nn.relu, name=None):\n",
    "        super(Dense, self).__init__(name=name) # remember this call to initialize the superclass\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "        self.is_built = False\n",
    "\n",
    "    def build(self, x):\n",
    "        input_dim = x.shape[-1]\n",
    "        self.w = tf.Variable(\n",
    "          tf.random.normal([input_dim, self.output_size]), name='w')\n",
    "        self.b = tf.Variable(tf.zeros([self.output_size]), name='b')\n",
    "        self.is_built = True\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        if not self.is_built:\n",
    "            self.build(x)\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        return self.activation(y)\n",
    "\n",
    "# Create an instance of the layer\n",
    "dense_layer = Dense(10)\n",
    "# Call the model by passing the input to it\n",
    "dense_layer(tf.ones([32,3]))\n",
    "# We can get the variables of a Module for used calculating and applying gradients with `.trainable_variables`\n",
    "dense_layer.trainable_variables\n",
    "\n",
    "# 32 x 3 (X)\n",
    "# 3 x 10 (W)\n",
    "# 10 (b)\n",
    "# keras.activations.relu(x, \n",
    "#                        negative_slope=0.0, \n",
    "#                        max_value=None, \n",
    "#                        threshold=0.0) => max(x, 0), the element-wise maximum of 0 and the input tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading TensorFlow models\n",
    "\n",
    "There are two main ways to save and load TF models: \n",
    "- `tf.train.Checkpoint`  \n",
    "- `tf.SavedModel`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `tf.train.Checkpoint`. \n",
    "- It's best used in the process of training rather than for saving models. \n",
    "- This is because it only saves and loads the variables, and **not the structure of the model**. \n",
    "\n",
    "To use it, you must first instantiate the model from Python code and then load the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir() # list contents o fthe currect working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The save path is ./tmp/training_checkpoints-1\n",
      "Raises an exception if any variables are unmatched. \n",
      " <tensorflow.python.training.tracking.util.CheckpointLoadStatus object at 0x14a1a8973f10>\n"
     ]
    }
   ],
   "source": [
    "# we have to pass in the model (and anything else we want to save) as a kwarg\n",
    "dense_layer = Dense(10)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "checkpoint = tf.train.Checkpoint(model=dense_layer, optimizer=optimizer)\n",
    "\n",
    "# Save a checkpoint to /tmp/training_checkpoints-{save_counter}. \n",
    "# Every time checkpoint.save is called, the save counter is increased.\n",
    "# below statement will create a /tmp dir inside the cirrent dir and save checkpoints inside it.\n",
    "save_dir = checkpoint.save('./tmp/training_checkpoints') \n",
    "# run os.listdir() and verify if the checkpointing folder has been created.\n",
    "\n",
    "# Restore the checkpointed values to the `model` object.\n",
    "print(\"The save path is\", save_dir)\n",
    "status = checkpoint.restore(save_dir)\n",
    "# we can check that everything loaded correctly, this is silent if all is well\n",
    "print(\"Raises an exception if any variables are unmatched. \\n\", status.assert_consumed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The save path is ./tmp/training_checkpoints-2\n",
      "The save path is ./tmp/training_checkpoints-3\n",
      "The save path is ./tmp/training_checkpoints-4\n",
      "The save path is ./tmp/training_checkpoints-5\n"
     ]
    }
   ],
   "source": [
    "# check out how successive checkpoints are saved\n",
    "for i in range(4):\n",
    "    save_dir = checkpoint.save('./tmp/training_checkpoints')\n",
    "    print(\"The save path is\", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `tf.SavedModel`\n",
    "\n",
    "- saves the variables and structure of the model. \n",
    "- Specifically, it only saves methods which have been traced with `tf.function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.548455  0.        1.0361035 0.        0.       ]\n",
      " [0.548455  0.        1.0361035 0.        0.       ]\n",
      " [0.548455  0.        1.0361035 0.        0.       ]], shape=(3, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# We can manually trace a function that has been decorated with tf.function using get_concrete_function\n",
    "# We pass in the call signature. Here, None means that any number could fill in.\n",
    "# Typically we don't have to do this explicitly, unless we want the None in the first dimension (as in the homework)\n",
    "dense_layer = Dense(5) # was 10, changed to 5 for demo\n",
    "\n",
    "# call using input spec\n",
    "fn = dense_layer.__call__.get_concrete_function(x=tf.TensorSpec([None, 3], tf.float32))\n",
    "\n",
    "# We can call the function we traced\n",
    "print(fn(tf.ones([3,3]))) # X=> 3x3; W=> 3x10; b=> 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model....\n",
      "INFO:tensorflow:Assets written to: ./tmp/saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving model....\")\n",
    "tf.saved_model.save(dense_layer, './tmp/saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(True) if \"dense_layer\" in locals() else print(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "del dense_layer\n",
    "\n",
    "print(True) if \"dense_layer\" in locals() else print(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.548455  0.        1.0361035 0.        0.       ]\n",
      " [0.548455  0.        1.0361035 0.        0.       ]\n",
      " [0.548455  0.        1.0361035 0.        0.       ]], shape=(3, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "restored_dense = tf.saved_model.load('./tmp/saved_model')\n",
    "\n",
    "# this should be the same result as above\n",
    "print(restored_dense(tf.ones([3,3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running TensorFlow-based Python programs on HCC\n",
    "\n",
    "#### 1. Get a shell on swan\n",
    "To access a shell on swan's login node, you can run `ssh <username>@swan.unl.edu` or visit `swan-ood.unl.edu` in a browser. If you use the browser, login and then use the dropdown: `Clusters > Swan Shell Access`. Because this shell is on the login node, you shouldn't run your jobs directly. Instead, you can submit your job to a cluster node to be run with a GPU.\n",
    "\n",
    "#### 2. Get the slurm submit script and set up your anaconda environment\n",
    "Try this out by saving the following bash script as `submit_gpu.sh`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/sh\n",
    "#SBATCH --time=6:00:00          # Maximum run time in hh:mm:ss\n",
    "#SBATCH --mem=16000             # Maximum memory required (in megabytes)\n",
    "#SBATCH --job-name=default_479  # Job name (to track progress)\n",
    "#SBATCH --partition=cse479      # Partition on which to run job\n",
    "#SBATCH --gres=gpu:1            # Don't change this, it requests a GPU\n",
    "#SBATCH --constraint=gpu_16gb   # will request a GPU with 16GB of RAM, independent of the type of card\n",
    "#SBATCH --licenses=common\n",
    "\n",
    "module load mamba\n",
    "conda activate /common/cse479/shared/envs/tensorflow-env\n",
    "$@\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Submit a job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've got your script, you can run it like so to submit a job: \n",
    "`sbatch submit_gpu.sh python <filepath>.py`. \n",
    "This will submit a job to the `cse479` partition. \n",
    "\n",
    "Each student is only allowed one job at a time on this partition, and you may check the status of your jobs with `squeue -u <username>`. \n",
    "\n",
    "If you would like to submit more than one job (which we encourage generally), you can submit the extras to the `cse479_preempt` partition by substituting in the line `#SBATCH --partition=cse479_preempt`. \n",
    "\n",
    "You can submit an unlimited number of jobs to this queue, but they may be interrupted anytime another student needs a gpu on the `cse479` partition and there are no others available. Thus, we reccomend saving your model periodically so that you can restart training from where it was interrupted rather than having to start all over again.\n",
    "\n",
    "Other partitions on swan are available, including `gpu` and `guest_gpu`.  See [Partitions](https://hcc.unl.edu/docs/submitting_jobs/partitions/) and [Available Partitions](https://hcc.unl.edu/docs/submitting_jobs/partitions/swan_available_partitions/).\n",
    "\n",
    "You can cancel jobs with `scancel <JOBID>`, where the job ID is the number associated with the job you can see in squeue or right after you submit the job, or with `scancel -u <username>` which cancels all your running jobs. For more details, please visit the [HCC docs](https://hcc.unl.edu/docs/), ask a question on Piazza, or come to office hours.\n",
    "\n",
    "[List of GPUs](https://hcc.unl.edu/docs/submitting_jobs/submitting_gpu_jobs/)\n",
    "\n",
    "[SLURM](https://hcc.unl.edu/docs/submitting_jobs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To check job status\n",
    "squeue -u <username>\n",
    "\n",
    "# To cancel a particular job\n",
    "scancel <JOBID>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting, regularization, and early stopping\n",
    "\n",
    "If we have enough parameters in our model, and little enough data, after a long period of training we begin to experience overfitting. Empirically, this is when the loss/classification error on the  training set continues to drop while the loss/classification error on the test/validation data increases. It means that the model is looking for patterns specific to the training data that won't generalize to future, unseen data. This is a problem.\n",
    "\n",
    "Solutions? Here are some first steps to think about:\n",
    "1. Get more data for the training set\n",
    "2. Reduce the number of model parameters\n",
    "3. Regularize the scale of the model parameters\n",
    "4. Regularize using dropout\n",
    "5. Early Stopping\n",
    "\n",
    "We'll go over how to do 3, 4, and 5 here.\n",
    "\n",
    "#### L2 Regularization\n",
    "We calculate l2 loss on the value of the weight matrix, so it's invariant to the input value. We'll add the regularization loss value to the total loss value so that it's included in the gradient update. \n",
    "\n",
    "[tf.nn.l2_loss](https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_COEFF = 0.1 # Controls how strongly to use regularization\n",
    "\n",
    "class L2DenseNetwork(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(L2DenseNetwork, self).__init__(name=name) # remember this call to initialize the superclass\n",
    "        self.dense_layer1 = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
    "        self.dense_layer2 = tf.keras.layers.Dense(10)\n",
    "        \n",
    "    def l2_loss(self):\n",
    "        # Make sure the network has been called at least once to initialize the dense layer kernels\n",
    "        return tf.nn.l2_loss(self.dense_layer1.kernel) + tf.nn.l2_loss(self.dense_layer2.kernel)\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        embed = self.dense_layer1(x) # 1x100, 100x32, 32\n",
    "        output = self.dense_layer2(embed) # 1x32, 32x10, 10\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[-0.55784094,  0.17359522,  0.7770922 ,  0.5898498 , -0.11045835,\n",
       "        -0.5591592 ,  0.14211914, -0.4492089 ,  0.26128727,  0.42045826]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining, creating and calling the network repeatedly will trigger a WARNING about re-tracing the function\n",
    "# So we'll check to see if the variable exists already\n",
    "if 'l2_dense_net' not in locals():\n",
    "    l2_dense_net = L2DenseNetwork()\n",
    "\n",
    "\n",
    "l2_dense_net(tf.ones([1, 100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = l2_dense_net.l2_loss()                     # calculate l2 regularization loss\n",
    "\n",
    "cross_entropy_loss = 0.                              # calculate the classification loss\n",
    "\n",
    "total_loss = cross_entropy_loss + L2_COEFF * l2_loss # and add to the total loss, then calculate gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout\n",
    "\n",
    "Let's re-specify the network with regularization from [dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout). The Dropout layer randomly sets values to 0 with a frequency of the `rate` input (given when the layer is constructed). Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutDenseNetwork(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(DropoutDenseNetwork, self).__init__(name=name) # remember this call to initialize the superclass\n",
    "        self.dense_layer1 = Dense(32)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        self.dense_layer2 = Dense(10, activation=tf.identity)\n",
    "\n",
    "    @tf.function # = graph mode\n",
    "    def __call__(self, x, is_training):\n",
    "        \n",
    "        embed = self.dense_layer1(x)\n",
    "        propn_zero_before = tf.reduce_mean(tf.cast(tf.equal(embed, 0.), tf.float32))\n",
    "        embed = self.dropout(embed, is_training)\n",
    "        propn_zero_after = tf.reduce_mean(tf.cast(tf.equal(embed, 0.), tf.float32))\n",
    "        # Note that in a tf.function, we have to use tf.print to print the value of tensors\n",
    "        tf.print('Zeros before and after:', propn_zero_before, \"and\", propn_zero_after)\n",
    "        # tf.print(\"tf.executing_eagerly(): \", tf.executing_eagerly()) # <------------------- uncomment this (1)\n",
    "        output = self.dense_layer2(embed)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros before and after: 0.46875 and 0.625\n",
      "Something to think about: why isn't the difference exactly equal to the proportion we passed to dropout?\n"
     ]
    }
   ],
   "source": [
    "# Defining, creating and calling the network repeatedly will trigger a WARNING about re-tracing the function\n",
    "# So we'll check to see if the variable exists already\n",
    "\n",
    "# del drop_dense_net # <------------------- uncomment this (2)\n",
    "if 'drop_dense_net' not in locals():\n",
    "    drop_dense_net = DropoutDenseNetwork()\n",
    "    \n",
    "\n",
    "drop_dense_net(tf.ones([1, 100]), tf.constant(True))\n",
    "print(\"Something to think about: why isn't the difference exactly equal to the proportion we passed to dropout?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', 'open', '_', '__', '___', '__session__', '_i', '_ii', '_iii', '_i1', '_i2', 'os', '_i3', '_3', '_i4', '_4', '_i5', 'print_function', 'np', 'tf', 'tfds', 'plt', 'tqdm', 'DATA_DIR', '_i6', '_i7', 'tf_cube', '_i8', 'concrete_function_fp', '_i9', 'concrete_function_int', '_9', '_i10', '_i11', '_i12', '_12', '_i13', '_i14', 'X', '_i15', 'Dense', '_15', '_i16', '_16', '_i17', '_17', '_i18', 'optimizer', 'checkpoint', 'save_dir', 'status', '_i19', 'i', '_i20', 'fn', '_i21', '_i22', '_i23', '_i24', '_i25', '_i26', 'restored_dense', '_i27', '_i28', '_i29', 'L2_COEFF', 'L2DenseNetwork', '_i30', 'l2_dense_net', '_30', '_i31', 'l2_loss', 'cross_entropy_loss', 'total_loss', '_i32', 'DropoutDenseNetwork', '_i33', 'drop_dense_net', 'sys', '_i34', '_i35', '_35', '_i36'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__main__'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals()['__name__']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early Stopping\n",
    "\n",
    "Each gradient descent update takes only a small step, so we want to look at each input datum many times. How do we know when to stop, though? We want to keep training until improvement stops, but because neural networks are non-linear, they might get worse before they get better, so we don't want to stop them after getting worse one time. We'll use the following code to do \"early stopping\" with patience. We pass in the validation loss (Note: make sure you use validation loss, not training loss. This is important) and `check` will tell us whether we should stop. It will return `True` after the loss hasn't improved for `patience` epochs.\n",
    "\n",
    "Although you might choose whether or not the last two regularizers are appropriate based on the problem, you should always use early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Should we stop training? False\n",
      "Early stopping has waited 0 epochs out of 2 at loss 1.4\n",
      "Training...\n",
      "Should we stop training? False\n",
      "Early stopping has waited 1 epochs out of 2 at loss 1.4\n",
      "Training...\n",
      "Should we stop training? False\n",
      "Early stopping has waited 0 epochs out of 2 at loss 1.2\n",
      "Training...\n",
      "Should we stop training? False\n",
      "Early stopping has waited 1 epochs out of 2 at loss 1.2\n",
      "Training...\n",
      "Should we stop training? False\n",
      "Early stopping has waited 2 epochs out of 2 at loss 1.2\n",
      "Training...\n",
      "Should we stop training? True\n",
      "Early stopping has waited 3 epochs out of 2 at loss 1.2\n"
     ]
    }
   ],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=2, epsilon=1e-4): # orig 5. changed to 2 for demo\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): how many epochs of not improving before stopping training\n",
    "            epsilon (float): minimum amount of improvement required to reset counter\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.epsilon = epsilon\n",
    "        self.best_loss = float('inf')\n",
    "        self.epochs_waited = 0\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Early stopping has waited {} epochs out of {} at loss {}\".format(self.epochs_waited, self.patience, self.best_loss)\n",
    "        \n",
    "    def check(self, loss):\n",
    "        \"\"\"\n",
    "        Call after each epoch to check whether training should halt\n",
    "        \n",
    "        Args:\n",
    "            loss (float): loss value from the most recent epoch of training\n",
    "            \n",
    "        Returns:\n",
    "            True if training should halt, False otherwise\n",
    "        \"\"\"\n",
    "        if loss < (self.best_loss - self.epsilon):\n",
    "            self.best_loss = loss\n",
    "            self.epochs_waited = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.epochs_waited += 1\n",
    "            return self.epochs_waited > self.patience\n",
    "            \n",
    "early_stop_module = EarlyStopping()\n",
    "# pass in validation loss at each training epoch\n",
    "print(\"Training...\")\n",
    "print(\"Should we stop training?\", early_stop_module.check(1.4))\n",
    "print(early_stop_module)\n",
    "print(\"Training...\")\n",
    "print(\"Should we stop training?\", early_stop_module.check(2.3))\n",
    "print(early_stop_module)\n",
    "print(\"Training...\")\n",
    "print(\"Should we stop training?\", early_stop_module.check(1.2))\n",
    "print(early_stop_module)\n",
    "print(\"Training...\")\n",
    "print(\"Should we stop training?\", early_stop_module.check(1.2))\n",
    "print(early_stop_module)\n",
    "print(\"Training...\")\n",
    "print(\"Should we stop training?\", early_stop_module.check(1.2))\n",
    "print(early_stop_module)\n",
    "print(\"Training...\")\n",
    "print(\"Should we stop training?\", early_stop_module.check(1.2))\n",
    "print(early_stop_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "Your homework is to complete the following two tasks:\n",
    "1. Make sure you're comfortable submitting jobs on swan and saving models. Submit a job to `cse479` and put the job id from the submission in a text file along with the other information given by `squeue`.\n",
    "2. Think about the question posed above about dropout: \"Why isn't the difference between the number of zeros before and after applying dropout exactly equal to the dropout proportion?\" Consider the network architecture and what operations were run before the dropout layer. Write a few sentences about this in the same text file as the previous question and submit to Canvas.\n",
    "\n",
    "In addition to your text file, submit your `.py` file that implements model saving.\n",
    "\n",
    "I'm expecting this to take about an hour (or less if you're experienced). Feel free to use any code from this or previous hackathons. If you don't understand how to do any part of this or if it's taking you longer than that, please let me know in office hours or by email (both can be found on the syllabus). I'm also happy to discuss if you just want to ask more questions about anything in this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python CSCE479 (tensorflow-env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
