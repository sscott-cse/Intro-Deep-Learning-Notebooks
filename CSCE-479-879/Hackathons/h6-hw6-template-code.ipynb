{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f2b4a3-f13a-4f78-abd0-ecfb27e1bdca",
   "metadata": {},
   "source": [
    "# Template code for Homework 6\n",
    "\n",
    "As you work through each subsection, you may notice that some lines of code have been intentionally left incomplete. Fill out the necessary code to progress through the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a12b20-9cc3-4447-8b28-72d6fd921427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start with our library imports...\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np                 # to use numpy arrays\n",
    "import tensorflow as tf            # to specify and run computation graphs\n",
    "import tensorflow_datasets as tfds # to load training data\n",
    "import matplotlib.pyplot as plt    # to visualize data and draw plots\n",
    "from tqdm import tqdm              # to track progress of loops\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import seaborn as sns\n",
    "\n",
    "DATA_DIR = './tensorflow-datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fedc8a-a184-4761-babe-82cd8551d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model and tokenizer from an available task specific checkpoint\n",
    "model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
    "\n",
    "tokenizer = # AutoTokenizer.from_pretrained(model_name)\n",
    "model = # TFAutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df51d3d-ceb8-4e0d-a74f-480360bd77d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prepare test and train datasets. \n",
    "\n",
    "At the end of this subsection, you will have \n",
    "- x_train, y_train\n",
    "- x_valid, y_valid\n",
    "- x_test, y_test\n",
    "\n",
    "Run the cells of this subsection as-is for the homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37c669-21ef-4f2f-996d-10bc6c76f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading SNLI dataset (Stanford Natural Language Inference dataset) from HuggingFace\n",
    "snli = load_dataset(\"stanfordnlp/snli\").shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66007d9b-0b35-4a94-9f18-5dd91ac7e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "snli.keys() # The dataset is downloaded as a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a142a45-f4bd-4857-a4e5-2eba332807f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a subset to avoid longer training time. Feel free to change the number of samples.\n",
    "snli_train = snli[\"train\"].take(1000)\n",
    "snli_valid = snli[\"validation\"].take(50)\n",
    "snli_test = snli[\"train\"].take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f784c2-3740-4879-8970-f121debe902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tuple (premise, hypothesis) to be used as input features (x_train) in model.fit()\n",
    "def preprocess_fn(sample):\n",
    "    sample[\"sentence_pair\"] = tuple((sample[\"premise\"].lower(), sample[\"hypothesis\"].lower()))\n",
    "    return sample\n",
    "\n",
    "snli_train = snli_train.map(preprocess_fn, remove_columns=[\"premise\", \"hypothesis\"])\n",
    "snli_valid = snli_valid.map(preprocess_fn, remove_columns=[\"premise\", \"hypothesis\"])\n",
    "snli_test = snli_test.map(preprocess_fn, remove_columns=[\"premise\", \"hypothesis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589fd3b-e453-4aa4-8ac5-887db2bbe6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use Pandas DataFrames to make our data processing easier\n",
    "snli_train_df = pd.DataFrame(snli_train)\n",
    "snli_valid_df = pd.DataFrame(snli_valid)\n",
    "snli_test_df = pd.DataFrame(snli_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378494da-4161-48a6-b54b-f373c53baebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out rows with incorrect labels\n",
    "snli_train_df = snli_train_df[snli_train_df[\"label\"] != -1]\n",
    "snli_valid_df = snli_valid_df[snli_valid_df[\"label\"] != -1]\n",
    "snli_test_df = snli_test_df[snli_test_df[\"label\"] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843982a-8476-4548-b301-ce712183b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f6f71-d90c-4f07-b1ea-0319fd530e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the sentence pairs through a tokenizer\n",
    "x_train = tokenizer(snli_train_df[\"sentence_pair\"].to_list(), padding=True, return_tensors=\"tf\").data\n",
    "x_valid = tokenizer(snli_valid_df[\"sentence_pair\"].to_list(), padding=True, return_tensors=\"tf\").data\n",
    "x_test = tokenizer(snli_test_df[\"sentence_pair\"].to_list(), padding=True, return_tensors=\"tf\").data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787db168-08c6-47b4-bb0c-3fdd2d3152c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing labels\n",
    "y_train = tf.constant(snli_train_df[\"label\"].to_list())\n",
    "y_valid = tf.constant(snli_valid_df[\"label\"].to_list())\n",
    "y_test = tf.constant(snli_test_df[\"label\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91023c41-ba76-4988-af2e-8c047c69ccd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c360ea5-c055-463b-aef9-7372f294b2b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Visualize attention by running a test sample through an untrained model\n",
    "\n",
    "- Some lines of code have been intentionally left incomplete. Fill out the necessary code to progress through Homework 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f853e-51b9-4644-b9d4-c06441bf5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass test data through untrained model\n",
    "outputs_before_training = # model(**x_test, return_dict=True, output_attentions=True, output_hidden_states=True)\n",
    "outputs_before_training.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a93084-50b8-4a28-bc5d-444abf3e7be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use logits for generating predictions\n",
    "Y_probas_before_training = # tf.keras.activations.softmax(outputs_before_training.logits)\n",
    "Y_pred_before_training = # tf.argmax(Y_probas_before_training, axis=1)\n",
    "\n",
    "print(Y_pred_before_training)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b1c99-0a23-4647-8fb0-4b33e732d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs_before_training[\"attentions\"]) # layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12331c7e-69c7-45a3-8c20-0223adbb44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_before_training[\"attentions\"][5].shape # batch, heads, seq, seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698fbf92-23b6-44ca-9217-74af25381029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select layer and sample, same as above\n",
    "# visualizing first sample, all attention heads of layer 0 of the encoder\n",
    "\n",
    "def visualize_attentions(attentions, layer=0, sample=0):\n",
    "    \n",
    "    layer_num=layer\n",
    "    sample_num=sample\n",
    "    attentions=outputs_before_training[\"attentions\"][layer_num][sample_num, :, :, :] # layer0, input-sample 0\n",
    "    attentions.shape\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = [12, 12]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=4, ncols=3, sharey=True) # 12 = 4x3\n",
    "    fig.subplots_adjust(wspace=0.01)\n",
    "    \n",
    "    tick_labels = tokenizer.convert_ids_to_tokens(x_test[\"input_ids\"][sample_num])\n",
    "    \n",
    "    i=0\n",
    "    for r in range(4):\n",
    "        for c in range(3):\n",
    "    \n",
    "            cbar=True if c==2 else False\n",
    "    \n",
    "            sns.heatmap(attentions[i],\n",
    "                        xticklabels=tick_labels,\n",
    "                        yticklabels=tick_labels,\n",
    "                        cmap=\"plasma\",\n",
    "                        ax=ax[r][c],\n",
    "                        cbar=cbar)\n",
    "            i+=1\n",
    "    \n",
    "    fig.subplots_adjust(wspace=0.001)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8442c157-d2b6-41c3-9de8-9c1ed4fcbeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attentions(attentions=outputs_before_training[\"attentions\"], layer=0, sample=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fcbe2e-b35e-4d93-a24b-e7a6db2bc8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5fc8d60-0ee2-450c-bb5a-f5f4c9fbba23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Train the model, then evaluate\n",
    "\n",
    "- Some lines of code have been intentionally left incomplete. Fill out the necessary code to progress through Homework 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d41c3f-0dea-41e7-8e14-818b8935114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "loss = #tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "callback = tf.keras.callbacks.EarlyStopping(patience=3) # feel free to experiment with different values of patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a88429-fe63-441c-b3ef-c9ffd015b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=20, validation_data=(x_valid, y_valid), callbacks=[callback], verbose=1) # feel free to experiment with different # epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2220645-a970-4130-9717-85886db7c083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52c09a19-b903-4a84-93fb-9c072ad75971",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Evaluate and visualize attention after finetuning\n",
    "\n",
    "- Some lines of code have been intentionally left incomplete. Fill out the necessary code to progress through Homework 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d16818-6326-4c21-bf88-e2b71d465f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the test set through the finetuned model\n",
    "outputs_after_training = # model(**x_test, return_dict=True, output_attentions=True, output_hidden_states=True)\n",
    "outputs_after_training.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1f9b4-85a4-446a-a55d-be0b540b75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_probas_after_training = #tf.keras.activations.softmax(outputs_after_training.logits)\n",
    "Y_pred_after_training = #tf.argmax(Y_probas_after_training, axis=1)\n",
    "print(Y_pred_after_training)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f894697-6765-4d60-be62-4716e5f02650",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs_after_training[\"attentions\"]) # layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a9d310-344b-41f3-80c6-46a49c6dc41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_after_training[\"attentions\"][5].shape # batch, heads, seq, seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22524c-1349-457e-aaa0-f8b31f9421f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attentions(attentions=outputs_after_training[\"attentions\"], layer=0, sample=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db86d17e-0bd2-4dc2-b2a6-332fd74d6717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python CSCE479 (tensorflow-env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
